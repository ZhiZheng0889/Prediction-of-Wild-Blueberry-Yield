---
title: "R Notebook"
output: html_notebook
---

```{r}
# 1. Load libraries
library(tidyverse)
library(caret)
library(randomForest)
library(httr)
library(readr)

# 2. Import data
train_url <- "https://www.dropbox.com/s/2nx73ix5vcwb13y/train.csv?dl=1"
test_url <- "https://www.dropbox.com/s/62xd4ajnwrqc7eg/test.csv?dl=1"

train_temp <- tempfile()
test_temp <- tempfile()

GET(train_url, write_disk(train_temp, overwrite = TRUE))
GET(test_url, write_disk(test_temp, overwrite = TRUE))

train <- read_csv(train_temp)
test <- read_csv(test_temp)

# 3. EDA
summary(train)
summary(test)
glimpse(train)
glimpse(test)
cor(train)

# 4. Data preparation
# Scaling/normalizing data
train_without_target <- train %>% select(-yield)
preProcess_range <- preProcess(train_without_target, method = c("range"))
train_without_target <- predict(preProcess_range, newdata = train_without_target)
test_processed <- predict(preProcess_range, newdata = test)

# Replace the original train dataset with the scaled version
train <- cbind(train_without_target, yield = train$yield)

```
```{r}
# 1. Load additional library
library(leaps)

# 2. Perform best subset selection using the 'regsubsets' function
formula <- yield ~ .
best_subset <- regsubsets(formula, data = train, nvmax = ncol(train) - 1)

# 3. Display the best model of each size
best_subset_summary <- summary(best_subset)
best_subset_summary$which

```
```{r}
# Obtain adjusted R-squared values for each model
best_subset_summary$adjr2

# Obtain BIC values for each model
best_subset_summary$bic

# Identify the variables in the 9-variable model
best_subset_model_9 <- which(best_subset_summary$which[9, ])
colnames(train)[best_subset_model_9]

# Fit a linear regression model using the 9-variable subset
formula_9 <- as.formula(yield ~ clonesize + honeybee + bumbles + andrena + osmia + MaxOfUpperTRange + MinOfUpperTRange + MinOfLowerTRange + AverageOfLowerTRange)
lm_9 <- lm(formula_9, data = train)
summary(lm_9)


```



```{r}
# Cross-validation
set.seed(123) # for reproducibility
cv_folds <- 10
lm_cv <- trainControl(method = "cv", number = cv_folds, savePredictions = "final")
formula_9 <- as.formula("yield ~ clonesize + honeybee + bumbles + andrena + osmia + MaxOfUpperTRange + MinOfUpperTRange + MinOfLowerTRange + AverageOfLowerTRange")
lm_model <- train(formula_9, data = train, method = "lm", trControl = lm_cv)

# Model performance during cross-validation
print(lm_model)

# Making predictions on test data
test_predictions <- predict(lm_model, newdata = test_processed)
print(test_predictions)

# Use the best model to make predictions on the test set
test_predictions <- predict(lm(formula_9, data = train), test_processed)

# Combine id from the test dataset and predicted yield from test_predictions
submission <- data.frame(id = test$id, yield = test_predictions)

# Save submission file as a CSV
write.csv(submission, "submission.csv", row.names = FALSE)


```
```{r}
# Load required libraries
library(glmnet)
library(randomForest)

# Define cross-validation parameters
set.seed(123) # for reproducibility
cv_folds <- 10
train_control <- trainControl(method = "cv", number = cv_folds, savePredictions = "final")

# Model training
# Linear Regression
lm_model <- train(formula_9, data = train, method = "lm", trControl = train_control)

# Ridge Regression
ridge_model <- train(formula_9, data = train, method = "ridge", trControl = train_control)

# Lasso Regression
lasso_model <- train(formula_9, data = train, method = "lasso", trControl = train_control)

# Elastic Net Regression
enet_model <- train(formula_9, data = train, method = "enet", trControl = train_control)

# Random Forest
rf_model <- train(formula_9, data = train, method = "rf", trControl = train_control)

# Model performance comparison
results <- resamples(list(linear = lm_model, ridge = ridge_model, lasso = lasso_model, enet = enet_model, rf = rf_model))
summary(results)
bwplot(results, layout = c(2, 1))

```
```{r}
# Load necessary libraries
library(randomForest)
library(caret)

# Set seed for reproducibility
set.seed(123)

# Create formula with the selected 13 variables
best_subset_formula <- as.formula("yield ~ clonesize + honeybee + bumbles + andrena + osmia + MaxOfUpperTRange + MinOfUpperTRange + MinOfLowerTRange + AverageOfLowerTRange + fruitset + fruitmass + seeds + id")

# Define cross-validation strategy
cv_folds <- 10
rf_cv <- trainControl(method = "cv", number = cv_folds, savePredictions = "final")

# Create Random Forest model using the best_subset_formula and ensemble (bagging)
rf_model <- train(best_subset_formula, data = train, method = "rf", trControl = rf_cv)

# Print model performance during cross-validation
print(rf_model)

# Make predictions on test dataset
test_predictions <- predict(rf_model, newdata = test_processed)

# Combine id from the test dataset and predicted yield from test_predictions
submission <- data.frame(id = test$id, yield = test_predictions)

# Save submission file as a CSV
write.csv(submission, "submission_rf_best_subset.csv", row.names = FALSE)

```
```{r}
# Load libraries
library(tidyverse)
library(caret)
library(randomForest)
library(xgboost)
library(httr)
library(readr)

# Import data
train_url <- "https://www.dropbox.com/s/2nx73ix5vcwb13y/train.csv?dl=1"
test_url <- "https://www.dropbox.com/s/62xd4ajnwrqc7eg/test.csv?dl=1"

train_temp <- tempfile()
test_temp <- tempfile()

GET(train_url, write_disk(train_temp, overwrite = TRUE))
GET(test_url, write_disk(test_temp, overwrite = TRUE))

train <- read_csv(train_temp)
test <- read_csv(test_temp)

# Data preparation
# Scaling/normalizing data
train_without_target <- train %>% select(-yield)
preProcess_range <- preProcess(train_without_target, method = c("range"))
train_without_target <- predict(preProcess_range, newdata = train_without_target)
test_processed <- predict(preProcess_range, newdata = test)

# Replace the original train dataset with the scaled version
train <- cbind(train_without_target, yield = train$yield)

# Best subset formula
best_subset_formula <- as.formula("yield ~ clonesize + honeybee + bumbles + andrena + osmia + MaxOfUpperTRange + MinOfUpperTRange + MinOfLowerTRange + AverageOfLowerTRange + fruitset + fruitmass + seeds + id")

# Create Random Forest model
set.seed(123)  # For reproducibility
rf_model <- randomForest(best_subset_formula, data = train, importance = TRUE)
print(rf_model)

# Make predictions on the test dataset with Random Forest
test_rf_predictions <- predict(rf_model, newdata = test_processed)

# Prepare data for XGBoost
best_subset_formula_xgb <- as.formula("~ clonesize + honeybee + bumbles + andrena + osmia + MaxOfUpperTRange + MinOfUpperTRange + MinOfLowerTRange + AverageOfLowerTRange")
train_matrix <- xgb.DMatrix(data = model.matrix(best_subset_formula_xgb, train), label = train$yield)
test_matrix <- xgb.DMatrix(data = model.matrix(best_subset_formula_xgb, test_processed))


# Train XGBoost model
set.seed(123)  # For reproducibility
xgb_params <- list(objective = "reg:linear")
xgb_model <- xgb.train(params = xgb_params, data = train_matrix, nrounds = 100)

# Make predictions on the test dataset with XGBoost
test_xgb_predictions <- predict(xgb_model, newdata = test_matrix)

# Combine the id column from the test dataset with the Random Forest and XGBoost predictions
rf_submission <- data.frame(id = test$id, yield = test_rf_predictions)
xgb_submission <- data.frame(id = test$id, yield = test_xgb_predictions)

# Save the Random Forest and XGBoost submission files as CSV
write.csv(rf_submission, "rf_submission.csv", row.names = FALSE)
write.csv(xgb_submission, "xgb_submission.csv", row.names = FALSE)

# Ensemble predictions (weighted average)
weight_rf <- 0.5
weight_xgb <- 1 - weight_rf
test_ensemble_predictions <- (weight_rf * test_rf_predictions) + (weight_xgb * test_xgb_predictions)

# Combine the id column from the test dataset with the ensemble predictions
ensemble_submission <- data.frame(id = test$id, yield = test_ensemble_predictions)

# Save the ensemble submission file as a CSV
write.csv(ensemble_submission, "ensemble_submission.csv", row.names = FALSE)



```
```{r}
# Load libraries
library(tidyverse)
library(caret)
library(randomForest)
library(xgboost)
library(httr)
library(readr)

# Import data
train_url <- "https://www.dropbox.com/s/2nx73ix5vcwb13y/train.csv?dl=1"
test_url <- "https://www.dropbox.com/s/62xd4ajnwrqc7eg/test.csv?dl=1"

train_temp <- tempfile()
test_temp <- tempfile()

GET(train_url, write_disk(train_temp, overwrite = TRUE))
GET(test_url, write_disk(test_temp, overwrite = TRUE))

train <- read_csv(train_temp)
test <- read_csv(test_temp)

# Data preparation
# Scaling/normalizing data
train_without_target <- train %>% select(-yield)
preProcess_range <- preProcess(train_without_target, method = c("range"))
train_without_target <- predict(preProcess_range, newdata = train_without_target)
test_processed <- predict(preProcess_range, newdata = test)

# Replace the original train dataset with the scaled version
train <- cbind(train_without_target, yield = train$yield)

# Best subset formula
best_subset_formula <- as.formula("yield ~ clonesize + honeybee + bumbles + andrena + osmia + MaxOfUpperTRange + MinOfUpperTRange + MinOfLowerTRange + AverageOfLowerTRange + fruitset + fruitmass + seeds + id")

# Create Random Forest model
set.seed(123)  # For reproducibility
rf_model <- randomForest(best_subset_formula, data = train, importance = TRUE)
print(rf_model)

# Make predictions on the test dataset with Random Forest
test_rf_predictions <- predict(rf_model, newdata = test_processed)

# Prepare data for XGBoost
best_subset_formula_xgb <- as.formula("~ clonesize + honeybee + bumbles + andrena + osmia + MaxOfUpperTRange + MinOfUpperTRange + MinOfLowerTRange + AverageOfLowerTRange")
train_matrix <- xgb.DMatrix(data = model.matrix(best_subset_formula_xgb, train), label = train$yield)
test_matrix <- xgb.DMatrix(data = model.matrix(best_subset_formula_xgb, test_processed))

# Train XGBoost model
set.seed(123)  # For reproducibility
xgb_params <- list(objective = "reg:linear")
xgb_model <- xgb.train(params = xgb_params, data = train_matrix, nrounds = 100)

# Make predictions on the test dataset with XGBoost
test_xgb_predictions <- predict(xgb_model, newdata = test_matrix)

# Combine the id column from the test dataset with the Random Forest and XGBoost predictions
rf_submission <- data.frame(id = test$id, yield = test_rf_predictions)
xgb_submission <- data.frame(id = test$id, yield = test_xgb_predictions)

# Save the Random Forest and XGBoost submission files as CSV
write.csv(rf_submission, "rf_submission.csv", row.names = FALSE)
write.csv(xgb_submission, "xgb_submission.csv", row.names = FALSE)

# Ensemble predictions (weighted average)
weight_rf <- 0.5
weight_xgb <- 1 - weight_rf
test_ensemble_predictions <- (weight_rf * test_rf_predictions) + (weight_xgb * test_xgb_predictions)

# Combine the id column from the test dataset with the ensemble predictions
ensemble_submission <- data.frame(id = test$id, yield = test_ensemble_predictions)

# Save the ensemble submission file as a CSV
write.csv(ensemble_submission, "ensemble_submission.csv", row.names = FALSE)


```

```{r}
# Load libraries
library(tidyverse)
library(caret)
library(randomForest)
library(httr)
library(readr)

# Import data
train_url <- "https://www.dropbox.com/s/2nx73ix5vcwb13y/train.csv?dl=1"
test_url <- "https://www.dropbox.com/s/62xd4ajnwrqc7eg/test.csv?dl=1"

train_temp <- tempfile()
test_temp <- tempfile()

GET(train_url, write_disk(train_temp, overwrite = TRUE))
GET(test_url, write_disk(test_temp, overwrite = TRUE))

train <- read_csv(train_temp)
test <- read_csv(test_temp)

# Data preparation
# Scaling/normalizing data
train_without_target <- train %>% select(-yield)
preProcess_range <- preProcess(train_without_target, method = c("range"))
train_without_target <- predict(preProcess_range, newdata = train_without_target)
test_processed <- predict(preProcess_range, newdata = test)

# Replace the original train dataset with the scaled version
train <- cbind(train_without_target, yield = train$yield)

# Best subset formula
best_subset_formula <- as.formula("yield ~ clonesize + honeybee + bumbles + andrena + osmia + MaxOfUpperTRange + MinOfUpperTRange + MinOfLowerTRange + AverageOfLowerTRange + fruitset + fruitmass + seeds + id")

# Tune the mtry parameter
set.seed(123)
tuned_rf <- tuneRF(x = train_without_target, y = train$yield,
                   ntreeTry = 500, stepFactor = 1.5, improve = 0.01,
                   trace = TRUE, plot = TRUE)

optimal_mtry <- as.numeric(names(which.min(tuned_rf[, "OOBError"])))

# Train the Random Forest model with the optimal mtry
set.seed(123)
rf_model_opt <- randomForest(best_subset_formula, data = train,
                             ntree = 500, mtry = optimal_mtry, importance = TRUE)
print(rf_model_opt)


# Make predictions on the test dataset with the optimized Random Forest model
test_rf_predictions_opt <- predict(rf_model_opt, newdata = test_processed)

# Combine the id column from the test dataset with the optimized Random Forest predictions
rf_submission_opt <- data.frame(id = test$id, yield = test_rf_predictions_opt)

# Save the optimized Random Forest submission file as a CSV
write.csv(rf_submission_opt, "rf_submission_opt.csv", row.names = FALSE)

```

```{r}
# Load libraries
library(tidyverse)
library(caret)
library(randomForest)
library(httr)
library(readr)

# Import data
train_url <- "https://www.dropbox.com/s/2nx73ix5vcwb13y/train.csv?dl=1"
test_url <- "https://www.dropbox.com/s/62xd4ajnwrqc7eg/test.csv?dl=1"

train_temp <- tempfile()
test_temp <- tempfile()

GET(train_url, write_disk(train_temp, overwrite = TRUE))
GET(test_url, write_disk(test_temp, overwrite = TRUE))

train <- read_csv(train_temp)
test <- read_csv(test_temp)

# Data preparation
# Scaling/normalizing data
train_without_target <- train %>% select(-yield)
preProcess_range <- preProcess(train_without_target, method = c("range"))
train_without_target <- predict(preProcess_range, newdata = train_without_target)
test_processed <- predict(preProcess_range, newdata = test)

# Replace the original train dataset with the scaled version
train <- cbind(train_without_target, yield = train$yield)

# Define the control parameters for RFE
control <- rfeControl(functions = rfFuncs,
                      method = "cv",
                      number = 5,
                      verbose = FALSE)

# Run RFE to find the optimal subset of features
set.seed(123)
rfe_result <- rfe(train[, -which(names(train) == "yield")],
                  train$yield,
                  sizes = 1:ncol(train), # Number of features to retain at each iteration
                  rfeControl = control)

# Check the results of RFE
print(rfe_result)
plot(rfe_result, type = c("g", "o"))

# Train a new Random Forest model using the selected features
selected_features <- names(rfe_result$optVariables)
formula_rf_rfe <- as.formula(paste("yield ~", paste(selected_features, collapse = " + ")))

# Tune the mtry parameter for the model with selected features
set.seed(123)
tuned_rf_rfe <- tuneRF(x = train[, selected_features], y = train$yield,
                       ntreeTry = 500, stepFactor = 1.5, improve = 0.01,
                       trace = TRUE, plot = TRUE)

optimal_mtry_rfe <- as.numeric(names(which.min(tuned_rf_rfe[, "OOBError"])))

# Train the Random Forest model with the optimal mtry and selected features
set.seed(123)
rf_model_opt_rfe <- randomForest(formula_rf_rfe, data = train,
                                 ntree = 500, mtry = optimal_mtry_rfe, importance = TRUE)
print(rf_model_opt_rfe)

# Make predictions on the test dataset with the optimized Random Forest model using RFE features
test_rf_predictions_opt_rfe <- predict(rf_model_opt_rfe, newdata = test_processed)

# Combine the id column from the test dataset with the optimized Random Forest predictions using RFE features
rf_submission_opt_rfe <- data.frame(id = test$id, yield = test_rf_predictions_opt_rfe)

# Save the optimized Random Forest submission file with RFE features as a CSV
write.csv(rf_submission_opt_rfe, "rf_submission_opt_rfe.csv", row.names = FALSE)

```

